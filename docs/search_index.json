[
["index.html", "Collaborative Data Science Practices Chapter 1 Prerequisites", " Collaborative Data Science Practices Will Beasley 2019-01-16 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["architecture.html", "Chapter 2 Architecture Principles 2.1 Encapsulation 2.2 Leverage team member’s strenghts &amp; avoid weaknesses 2.3 Scales 2.4 Consistency", " Chapter 2 Architecture Principles 2.1 Encapsulation 2.2 Leverage team member’s strenghts &amp; avoid weaknesses Focused code files Metadata for content experts 2.3 Scales Single source &amp; single analysis Multiple sources &amp; multiple analyses 2.4 Consistency Across Files {#consistency-files} Across Languages Across Projects "],
["file-prototype.html", "Chapter 3 Prototypical File 3.1 Clear Memory 3.2 Load Sources 3.3 Load Packages 3.4 Declare Globals 3.5 Load Data 3.6 Tweak Data 3.7 (Unique Content) 3.8 Verify Values 3.9 Specify Output Columns 3.10 Save to Disk or Database", " Chapter 3 Prototypical File As stated before, in Consistency Files, using a consistent file structure can (a) improve the quality of the code because the structure has been proven over time to facilitate good practices and (b) allow your intentions to be more clear to teammates because they are familiar with the order and intentions of the chunks. We use the term “chunk” for a section of code because it corresponds with knitr terminology (Xie 2015), and in many cases, the chunk of our R file connects to a knitr Rmd file. 3.1 Clear Memory 3.2 Load Sources 3.3 Load Packages 3.4 Declare Globals 3.5 Load Data 3.6 Tweak Data 3.7 (Unique Content) 3.8 Verify Values 3.9 Specify Output Columns 3.10 Save to Disk or Database References "],
["repo-prototype.html", "Chapter 4 Prototypical Repository 4.1 Analysis 4.2 Data Public 4.3 Data Unshared 4.4 Documentation 4.5 Manipulation 4.6 Stitched Output 4.7 Utility", " Chapter 4 Prototypical Repository https://github.com/wibeasley/RAnalysisSkeleton 4.1 Analysis 4.2 Data Public Raw Derived Metadata Database Original 4.3 Data Unshared 4.4 Documentation 4.5 Manipulation 4.6 Stitched Output 4.7 Utility "],
["data-at-rest.html", "Chapter 5 Data at Rest 5.1 Data States 5.2 Data Containers", " Chapter 5 Data at Rest 5.1 Data States Raw Derived Project-wide File on Repo Project-wide File on Protected File Server User-specific File on Protected File Server Project-wide Database Original 5.2 Data Containers csv rds SQLite Central Enterprise database Central REDCap database Containers to avoid for raw/input Proprietary like xlsx, sas7bdat "],
["patterns.html", "Chapter 6 Patterns 6.1 Ellis 6.2 Arch 6.3 Ferry 6.4 Scribe 6.5 Analysis 6.6 Presentation -Static 6.7 Presentation -Interactive 6.8 Metadata", " Chapter 6 Patterns 6.1 Ellis 6.2 Arch 6.3 Ferry 6.4 Scribe 6.5 Analysis 6.6 Presentation -Static 6.7 Presentation -Interactive 6.8 Metadata "],
["security.html", "Chapter 7 Security &amp; Private Data 7.1 File-level permissions 7.2 Database permissions 7.3 Public &amp; Private Repositories", " Chapter 7 Security &amp; Private Data 7.1 File-level permissions 7.2 Database permissions 7.3 Public &amp; Private Repositories 7.3.1 Scrubbing GitHub history Occassionaly files may be committed to your git repository that need to be removed completely. Not just from the current collections of files (i.e., the branch’s head), but from the entire history of the repo. Scrubbing is require typically when (a) a sensitive file has been accidentally commited and pushed to GitHub, or (b) a huge file has bloated your repository and disrupted productivity. The two suitable scrubbing approaches both require the command line. The first is the git-filter-branch command within git, and the second is the BFG repo-cleaner. We use the second approach, which is [recommended by GitHub]; it requires 15 minutes to install and configure from scratch, but then is much easier to develop against, and executes much faster. The bash-centric steps below remove any files from the repo history called ‘monster-data.csv’ from the ‘bloated’ repository. If the file contains passwords, change them immediately. Delete ‘monster-data.csv’ from your branch and push the commit to GitHub. Ask your collaborators to push any outstanding commits to GitHub and delete their local copy of the repo. Once scrubbing is complete, they will re-clone it. Download and install the most recent Java JRE from the Oracle site. Download the most recent jar file from the BFG site to the home directory. Clone a fresh copy of the repository in the user’s home directory. The --mirror argument avoids downloading every file, and downloads only the bookkeeping details required for scrubbing. cd ~ git clone --mirror https://github.com/your-org/bloated.git Remove all files (in any directory) called ‘monster-data.csv’. java -jar bfg-*.jar --delete-files monster-data.csv Reflog and garbage collect the repo. cd bloated.git git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive Push your local changes to the GitHub server. push Delete the bfg jar from the home directory. cd ~ rm bfg-*.jar Ask your collaborators to reclone the repo to their local machine. It is important they restart with a fresh copy, so the once-scrubbed file is not reintroduced into the repo’s history. If the file contains sensitive information, like passwords or PHI, ask GitHub to refresh the cache so the file’s history isn’t accessible through their website, even if the repo is private. 7.3.1.0.1 Resources BFG Repo-Cleaner site Additional BFG instructions GitHub Sensitive Data Removal Policy "],
["automation.html", "Chapter 8 Automation 8.1 Flow File in R 8.2 Makefile 8.3 SSIS 8.4 cron Jobs &amp; Task Scheduler 8.5 Sink Log Files", " Chapter 8 Automation 8.1 Flow File in R 8.2 Makefile 8.3 SSIS 8.4 cron Jobs &amp; Task Scheduler 8.5 Sink Log Files "],
["scaling-up.html", "Chapter 9 Scaling Up 9.1 Data Storage 9.2 Data Processing", " Chapter 9 Scaling Up 9.1 Data Storage Local File vs Conventional Database vs Redshift Usage Cases 9.2 Data Processing R vs SQL R vs Spark "],
["collaboration.html", "Chapter 10 Parallel Collaboration 10.1 Social Contract 10.2 Code Reviews 10.3 Remote", " Chapter 10 Parallel Collaboration 10.1 Social Contract Issues Organized Commits &amp; Coherent Diffs Branch &amp; Merge Strategy 10.2 Code Reviews Daily Reviews of PRs Periodic Reviews of Files 10.3 Remote Headset &amp; sharing screens "],
["document.html", "Chapter 11 Documentation 11.1 Team-wide 11.2 Project-specific 11.3 Dataset Origin &amp; Structure 11.4 Issues &amp; Tasks 11.5 Flow Diagrams 11.6 Setting up new machine", " Chapter 11 Documentation 11.1 Team-wide 11.2 Project-specific 11.3 Dataset Origin &amp; Structure 11.4 Issues &amp; Tasks 11.5 Flow Diagrams 11.6 Setting up new machine (example) "],
["publication.html", "Chapter 12 Publishing Results 12.1 To Other Analysts 12.2 To Researchers &amp; Content Experts 12.3 To Technical-Phobic Audiences", " Chapter 12 Publishing Results 12.1 To Other Analysts 12.2 To Researchers &amp; Content Experts 12.3 To Technical-Phobic Audiences "],
["testing-and-validation.html", "Chapter 13 Testing, Validation, &amp; Defensive Programming 13.1 Testing Functions 13.2 Defensive Programming 13.3 Validator", " Chapter 13 Testing, Validation, &amp; Defensive Programming 13.1 Testing Functions 13.2 Defensive Programming Throwing errors 13.3 Validator Benefits for Analysts Benefits for Data Collectors "],
["troubleshooting.html", "Chapter 14 Troubleshooting and Debugging 14.1 Finding Help 14.2 Debugging", " Chapter 14 Troubleshooting and Debugging 14.1 Finding Help Within your group (eg, Thomas and REDCap questions) Within your university (eg, SCUG) Outside (eg, Stack Overflow; GitHub issues) 14.2 Debugging traceback(), browser(), etc "],
["establishing-workstation.html", "Chapter 15 Considerations when Selecting Tools 15.1 Required Installation 15.2 Recommended Installation 15.3 Optional Installation 15.4 Asset Locations", " Chapter 15 Considerations when Selecting Tools https://github.com/OuhscBbmc/RedcapExamplesAndPatterns/blob/master/DocumentationGlobal/ResourcesInstallation.md 15.1 Required Installation 15.2 Recommended Installation 15.3 Optional Installation 15.4 Asset Locations "],
["tools.html", "Chapter 16 Considerations when Selecting Tools 16.1 General 16.2 Languages 16.3 R Packages 16.4 Database", " Chapter 16 Considerations when Selecting Tools 16.1 General 16.1.1 The Component’s Goal While disussing the advantages and disadvanages of tools, a colleague once said, “Tidyverse packages don’t do anything that I can’t already do in Base R, and sometimes it even requires more lines of code”. Regardless if I agree, I feel these two points are irrelevant. Sometimes the advantage of a tool isn’t to expand existing capabilities, but rather to facilitate development and maintaince for the same capability. Likewise, I care less about the line count, and more about the readability. I’d prefer to maintain a 20-line chunk that is familar and readable than a 10-line chunk with dense phrases and unfamiliar functions. The bottleneck for most of our projects is human time, not execution time. 16.1.2 Current Skillset of Team 16.1.3 Desired Future Skillset of Team 16.1.4 Skillset of Audience 16.2 Languages 16.3 R Packages 16.4 Database "],
["team.html", "Chapter 17 Growing a Team 17.1 Recruiting 17.2 Training to Data Science 17.3 Bridges Outside the Team", " Chapter 17 Growing a Team 17.1 Recruiting 17.2 Training to Data Science Starting with a Researcher Starting with a Statistician Starting with a DBA Starting with a Software Developer 17.3 Bridges Outside the Team Monthly User Groups Annual Conferences "],
["intro.html", "Chapter 18 Introduction", " Chapter 18 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 18. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 2. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 18.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 18.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 18.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 18.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["scratch-pad.html", "Chapter 19 Scratch Pad of Loose Ideas 19.1 Chapters &amp; Sections to Form", " Chapter 19 Scratch Pad of Loose Ideas 19.1 Chapters &amp; Sections to Form Tools to Consider tidyverse odbc "],
["references.html", "References", " References "]
]
