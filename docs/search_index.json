[
["index.html", "Collaborative Data Science Practices 1 Introduction", " Collaborative Data Science Practices Will Beasley 2019-12-13 1 Introduction This collection of documents describe practices used by the OUHSC BBMC in our analytics projects. "],
["coding.html", "2 Coding Principles 2.1 Simplify", " 2 Coding Principles 2.1 Simplify 2.1.1 Data Types Use the simplest data type reasonable. A simpler data type is less likely contain unintented values. As we have seen, a string variable called gender can simulatneously contain the values “m”, “f”, “F”, “Female”, “MALE”, “0”, “1”, “2”, “Latino”, \"\", and NA. On the other hand, a boolean variable gender_male can be only FALSE, TRUE, and NA.1. Once you have cleaned a variable in your initial ETL files (like an Ellis), lock it down so you do not have to spend time in the downstream files verifying that no bad values have been introduced. As a small bonus, simpler data types are typically faster, consume less memory, and translate more cleanly across platforms. Within R, the preference for numeric-ish variables is logical/boolean/bit, integer, bit64::integer64, and numeric/double-precision floats. The preference for categorical variables is logical/boolean/bit, factor, and character. 2.1.2 Categorical Levels When a boolean variable would be too restrictive and a factor or character is required, choose the simplest representation. Where possible: Use only lower case (e.g., ‘male’ instead of ‘Male’ for the gender variable). avoid repeating the variable in the level (e.g., ‘control’ instead of ‘control condition’ for the condition variable). 2.1.3 Recoding Almost every project recodes many variables. Choose the simplest function possible. The functions at the top are much easier to read and harder to mess up. !: instead of gender_male == FALSE or gender_male != TRUE, write !gender_male. dplyr::coalesce(): The function evaluates a single variable and replaces NA with values from another variable. A coalesce like visit_completed = dplyr::coalesce(visit_completed, FALSE) is much easier to read and not mess up than visit_completed = dplyr::if_else(!is.na(visit_completed), visit_completed, FALSE) dplyr::if_else(): The function evaluates a single boolean variable. The output branches to only three possiblities: condition is (a) true, (b) false, or (c) (optionally) NA. cut(): The function evaluations only a single numeric variable. It’s range is cut into different segments/categories on the one-dimensional number line. The output branches to single discrete value (either a factor-level or an integer). dplyr::recode(): The function evaluates a single integer or character variable. The output branches to a single discrete value. lookup table: It feasible recode 6 levels of race directly in R. It’s less feasible to recode 200 provider names. Specify the mapping in a csv, readr the csv to a data.frame, and left-join it. dplyr::case_when(): The function is the most complicated because it can evaluate multiple variables. Also, multiple cases can be true, but only the first output is returned. This ‘water fall’ execution helps in complicated scenarios, but is overkill for most. The database bit equivalents are 0, 1, and NULL↩︎ "],
["architecture.html", "3 Architecture Principles 3.1 Encapsulation 3.2 Leverage team member’s strenghts &amp; avoid weaknesses 3.3 Scales 3.4 Consistency 3.5 Defensive Style", " 3 Architecture Principles 3.1 Encapsulation 3.2 Leverage team member’s strenghts &amp; avoid weaknesses 3.2.1 Focused code files 3.2.2 Metadata for content experts 3.3 Scales 3.3.1 Single source &amp; single analysis 3.3.2 Multiple sources &amp; multiple analyses 3.4 Consistency 3.4.1 Across Files 3.4.2 Across Languages 3.4.3 Across Projects 3.5 Defensive Style Some of these may not be not be an ‘architecture’, but it guides many of our architectural principles. 3.5.1 Qualify functions Try to prepend each function with its package. Write dplyr::filter() instead of filter(). When two packages contain public functions with the same name, the package that was most recently called with library() takes precedent. When multiple R files are executed, the packages’ precedents may not be predictable. Specifying the package eliminates the ambiguity, while also making the code easier to follow. For this reason, we recommend that almost all R files contain a ‘load-packages’ chunk. "],
["file-prototype.html", "4 Prototypical File 4.1 Clear Memory 4.2 Load Sources 4.3 Load Packages 4.4 Declare Globals 4.5 Load Data 4.6 Tweak Data 4.7 (Unique Content) 4.8 Verify Values 4.9 Specify Output Columns 4.10 Save to Disk or Database", " 4 Prototypical File As stated before, in Consistency across Files, using a consistent file structure can (a) improve the quality of the code because the structure has been proven over time to facilitate good practices and (b) allow your intentions to be more clear to teammates because they are familiar with the order and intentions of the chunks. We use the term “chunk” for a section of code because it corresponds with knitr terminology (Xie 2015), and in many analysis files (as opposed to manipulation files), the chunk of our R file connects to a knitr Rmd file. 4.1 Clear Memory Before the initial chunk many of our files clear the memory of variables from previous run. This is important when developig and debugging because it prevents previous runs from contaminating subsequent runs. However it has little effect during production; we’ll look at manipulation files separately from analysis files. Manipulation R files are sourced with the argument local=new.env(). The file is executed in a fresh environment, so there are no variables to clear. Analysis R files are typically called from an Rmd file’s knitr::read_chunk(), and code positioned above the first chunk is not called by knitr.2 However typically do not clear the memory in R files that are sourced in the same environment as the caller, as it will interfere with the caller’s variables. rm(list = ls(all.names = TRUE)) 4.2 Load Sources In the first true chunk, source any R files containing global variables and functions that the current file requires. For instance, when a team of statisticians is producing a large report containing many analysis files, we define many of the graphical elements in a single file. This sourced file defines common color palettes and graphical functions so the cosmetics are more uniform across analyses. We prefer not to have sourced files perform any real action, such as importing data or manipulating a file. One reason is because it is difficult to be consistent about the environmental variables when the sourced file’s functions are run. A second reason is that it more cognitively difficult to understand how the files are connected. When the sourced file contains only function definitions, these operations can be called at any time in the current file with much tighter control of which variables are modfied. A bonus of the discipline of defining functions (instead of executing functions) is that the operations are typically more robust and generalizable. Keep the chunk even if no files are sourced. An empty chunk is instructive to readers trying to determine if any files are sourced. This applies recommendation applies to all the chunks discussed in this chapter. As always, your team should agree on its own set of standards. # ---- load-sources ------------------------------------------------------------ base::source(file=&quot;./analysis/common/display-1.R&quot;) # Load common graphing functions. 4.3 Load Packages The ‘load-packages’ chunk declares required packages near the file’s beginning for three reasons. First, a reader scanning the file can quickly determine its dependencies when located in a single chunk. Second, if your machine is lacking a required package, it is best to know early3. Third, this style mimics a requirement of other languages (such as declaring headers at the top of a C++ file) and follows the tidyverse style guide. As discussed in the previous qualify all functions section, we recommend that functions are qualified with their package (e.g., foo::bar() instead of merely bar()). Consequently, the ‘load-packages’ chunk calls requireNamespace() more frequently than library(). requireNamespace() verifies the package is available on the local machine, but does not load it into memory; library() verifies the package is available, and then loads it. requireNamespace() is not used in several scenarios. Core packages (e.g., ‘base’ and ‘stats’) are loaded by R in most default installations. We avoid unnecessary calls like library(stats) because they distract from more important features. Obvious dependencies are not called by reqiureNamespace() or library() for similar reasons, especially if they are not called directly. For example ‘tidyselect’ is not listed when ‘tidyr’ is listed. The “pipe” function (declared in the magrittr' package , *i.e.*,%&gt;%) is attached withimport::from(magrittr, “%&gt;%”)`. This frequently-used function called be called throughout the execution without qualification. Compared to manipulation files, our analysis files tend to use many functions in a few concentrated packages so conflicting function names are less common. Typical packages used in analysis are ‘ggplot2’ and ‘lme4’. The sourced files above may load their own packages (by calling library()). It is important that the library() calls in this file follow the ‘load-sources’ chunk so that identically-named functions (in different packages) are called with the correct precedent. Otherwise identically-named functions will conflict in the namespace with hard-to-predict results. Read R Packages for more about library(), requireNamespace(), and their siblings, as well as the larger concepts such as attaching functions into the search path. Here are packages found in most of our manipulation files. Notice the lesser-known packages have a quick explanation; this helps maintainers decide if the declaration is still necessary. Also notice the packages distributed outside of CRAN (e.g., GitHub) have a quick commented line to help the user install or update the package. # ---- load-packages ----------------------------------------------------------- import::from(magrittr, &quot;%&gt;%&quot; ) requireNamespace(&quot;readr&quot; ) requireNamespace(&quot;tidyr&quot; ) requireNamespace(&quot;dplyr&quot; ) requireNamespace(&quot;config&quot; ) requireNamespace(&quot;checkmate&quot; ) # Asserts expected conditions requireNamespace(&quot;OuhscMunge&quot;) # remotes::install_github(repo=&quot;OuhscBbmc/OuhscMunge&quot;) 4.4 Declare Globals When values are repeatedly used within a file, consider dedicating a variable so it’s defined and set only once. This is also a good place for variables that are used only once, but whose value are central to the file’s mission. Typical variables in our ‘declare-globals’ chunk include data file paths, data file variables, color palettes, and values in the config file. The config file can coordinate a static variable across multiple files. Centrally # ---- declare-globals --------------------------------------------------------- # Constant values that won&#39;t change. config &lt;- config::get() path_db &lt;- config$path_database # Execute to specify the column types. It might require some manual adjustment (eg doubles to integers). # OuhscMunge::readr_spec_aligned(config$path_subject_1_raw) col_types &lt;- readr::cols_only( subject_id = readr::col_integer(), county_id = readr::col_integer(), gender_id = readr::col_double(), race = readr::col_character(), ethnicity = readr::col_character() ) 4.5 Load Data All data ingested by this file occurs in this chunk. We like to think of each file as a linear pipe with a single point of input and single point of output. Although it is possible for a file to read data files on any line, we recommend avoiding this sprawl because it is more difficult for humans to understand. If the software developer is a deist watchmaker, the file’s fate has been sealed by the end of this chunk. This makes is easier for a human to reason to isolate problems as either existing with (a) the incoming data or (b) the calculations on that data. Ideally this chunk consumes data from either a plain-text csv or a database. Many capable R functions and packages ingest data. We prefer the tidyverse readr for reading conventional files; its younger cousin, vroom has some nice advantages when working with larger files and some forms of jagged rectangles4. 4.6 Tweak Data 4.7 (Unique Content) 4.8 Verify Values 4.9 Specify Output Columns 4.10 Save to Disk or Database Read more about knitr’s code externalization↩︎ The error message “Error in library(foo) : there is no package called ‘foo’” is easier to understand than “Error in bar() : could not find function ‘bar’” thrown somewhere in the middle of the file; this check can also illuminate conflicts arising when two packages have a bar() function. See McConnell 2004 Section qqq for more about the ‘fail early’ principle.↩︎ Say a csv has 20 columns, but a row has missing values for the last five columns. Instead of five successive commas to indicate five empty cells exist, a jagged rectangle simply ends after the last nonmissing value. vroom infers the missing values correctly, while some other packages do not.↩︎ "],
["repo-prototype.html", "5 Prototypical Repository 5.1 Root 5.2 Analysis 5.3 Data Public 5.4 Data Unshared 5.5 Documentation 5.6 Manipulation 5.7 Stitched Output 5.8 Utility", " 5 Prototypical Repository https://github.com/wibeasley/RAnalysisSkeleton 5.1 Root config.R is simply a plain-text yaml file read by the config package. It is great when a value has to be coordinated across multiple file default: # To be processed by Ellis lanes path_subject_1_raw: &quot;data-public/raw/subject-1.csv&quot; path_mlm_1_raw: &quot;data-public/raw/mlm-1.csv&quot; # Central Database (produced by Ellis lanes). path_database: &quot;data-public/derived/db.sqlite3&quot; # Analysis-ready datasets (produced by scribes &amp; consumed by analyses). path_mlm_1_derived: &quot;data-public/derived/mlm-1.rds&quot; # Metadata path_annotation: &quot;data-public/metadata/cqi-annotation.csv&quot; # Logging errors and messages from automated execution. path_log_flow: !expr strftime(Sys.time(), &quot;data-unshared/log/flow-%Y-%m-%d--%H-%M-%S.log&quot;) # time_zone_local : &quot;America/Chicago&quot; # Force local time, in case remotely run. # ---- Validation Ranges &amp; Patterns ---- range_record_id : !expr c(1L, 999999L) range_dob : !expr c(as.Date(&quot;2010-01-01&quot;), Sys.Date() + lubridate::days(1)) range_datetime_entry : !expr c(as.POSIXct(&quot;2019-01-01&quot;, tz=&quot;America/Chicago&quot;), Sys.time()) max_age : 25 pattern_mrn_centricity : &quot;^\\\\d{16}$&quot; # The 64-bit int is more easily validated as a character. flow.R README.md *.Rproj 5.2 Analysis 5.3 Data Public Raw Derived Metadata Database Original 5.4 Data Unshared 5.5 Documentation 5.6 Manipulation 5.7 Stitched Output 5.8 Utility "],
["data-at-rest.html", "6 Data at Rest 6.1 Data States 6.2 Data Containers", " 6 Data at Rest 6.1 Data States Raw Derived Project-wide File on Repo Project-wide File on Protected File Server User-specific File on Protected File Server Project-wide Database Original 6.2 Data Containers csv rds SQLite Central Enterprise database Central REDCap database Containers to avoid for raw/input Proprietary like xlsx, sas7bdat "],
["patterns.html", "7 Patterns 7.1 Ellis 7.2 Arch 7.3 Ferry 7.4 Scribe 7.5 Analysis 7.6 Presentation -Static 7.7 Presentation -Interactive 7.8 Metadata", " 7 Patterns 7.1 Ellis 7.1.1 Purpose To incorporate outside data source into your system safely. 7.1.2 Philosophy Without data immigration, all warehouses are useless. Embrace the power of fresh information in a way that is: repeatable when the datasource is updated (and you have to refresh your warehouse) similar to other Ellis lanes (that are designed for other data sources) so you don’t have to learn/remember an entirely new pattern. (Like Rubiks cube instructions.) 7.1.3 Guidelines Take small bites. Like all software development, don’t tackle all the complexity the first time. Start by processing only the important columns before incorporating move. Use only the variables you need in the short-term, especially for new projects. As everyone knows, the variables from the upstream source can change. Don’t spend effort writing code for variables you won’t need for a few months/years; they’ll likely change before you need them. After a row passes through the verify-values chunk, you’re accountable for any failures it causes in your warehouse. All analysts know that external data is messy, so don’t be surprised. Sometimes I’ll spend an hour writing an Ellis for 6 columns. Narrowly define each Ellis lane. One code file should strive to (a) consume only one CSV and (b) produce only one table. Exceptions include: if multiple input files are related, and really belong together (e.g., one CSV per month, or one CSV per clinic). This scenario is pretty common. if the CSV should legitimately produce two different tables after munging. This happens infrequently, such as one warehouse table needs to be wide, and another long. 7.1.4 Examples https://github.com/wibeasley/RAnalysisSkeleton/blob/master/manipulation/te-ellis.R https://github.com/wibeasley/RAnalysisSkeleton/blob/master/manipulation/ https://github.com/OuhscBbmc/usnavy-billets/blob/master/manipulation/survey-ellis.R 7.1.5 Elements Clear memory In scripting languages like R (unlike compiled languages like Java), it’s easy for old variables to hang around. Explicitly clear them before you run the file again. rm(list=ls(all=TRUE)) #Clear the memory of variables from previous run. This is not called by knitr, because it&#39;s above the first chunk. Load Sources In R, a source()d file is run to execute its code. We prefer that a sourced file only load variables (like function definitions), instead of do real operations like read a dataset or perform a calculation. There are many times that you want a function to be available to multiple files in a repo; there are two approaches we like. The first is collecting those common functions into a single file (and then sourcing it in the callers). The second is to make the repo a legitimate R package. The first approach is better suited for quick &amp; easy development. The second allows you to add documention and unit tests. # ---- load-sources ------------------------------------------------------------ source(&quot;./manipulation/osdh/ellis/common-ellis.R&quot;) Load Packages This is another precaution necessary in a scripting language. Determine if the necessary packages are available on the machine. Avoiding attaching packages (with the library() function) when possible. Their functions don’t need to be qualified (e.g., dplyr::intersect()) and could cause naming conflicts. Even if you can guarantee they don’t conflict with packages now, packages could add new functions in the future that do conflict. # ---- load-packages ----------------------------------------------------------- # Attach these package(s) so their functions don&#39;t need to be qualified: http://r-pkgs.had.co.nz/namespace.html#search-path library(magrittr , quietly=TRUE) library(DBI , quietly=TRUE) # Verify these packages are available on the machine, but their functions need to be qualified: http://r-pkgs.had.co.nz/namespace.html#search-path requireNamespace(&quot;readr&quot; ) requireNamespace(&quot;tidyr&quot; ) requireNamespace(&quot;dplyr&quot; ) # Avoid attaching dplyr, b/c its function names conflict with a lot of packages (esp base, stats, and plyr). requireNamespace(&quot;testit&quot;) requireNamespace(&quot;checkmate&quot;) requireNamespace(&quot;OuhscMunge&quot;) #devtools::install_github(repo=&quot;OuhscBbmc/OuhscMunge&quot;) Declare Global Variables and Functions. This includes defining the expected column names and types of the data sources; use readr::cols_only() (as opposed to readr::cols()) to ignore any new columns that may be been added since the dataset’s last refresh. # ---- declare-globals --------------------------------------------------------- Load Data Source(s) Read all data (e.g., database table, networked CSV, local lookup table). After this chunk, no new data should be introduced. This is for the sake of reducing human cognition load. Everything below this chunk is derived from these first four chunks. # ---- load-data --------------------------------------------------------------- Tweak Data # ---- tweak-data -------------------------------------------------------------- Body of the Ellis Verify # ---- verify-values ----------------------------------------------------------- county_month_combo &lt;- paste(ds$county_id, ds$month) checkmate::assert_character(county_month_combo, pattern =&quot;^\\\\d{1,2} \\\\d{4}-\\\\d{2}-\\\\d{2}$&quot;, any.missing=F, unique=T) Specify Columns Define the exact columns and order to upload to the database. Once you import a column into a warehouse that multiple people are using, it’s tough to remove it. # ---- specify-columns-to-upload ----------------------------------------------- Welcome into your warehouse. Until this chunk, nothing should be persisted. # ---- upload-to-db ------------------------------------------------------------ # ---- save-to-disk ------------------------------------------------------------ 7.2 Arch 7.3 Ferry 7.4 Scribe 7.5 Analysis 7.6 Presentation -Static 7.7 Presentation -Interactive 7.8 Metadata "],
["security.html", "8 Security &amp; Private Data 8.1 File-level permissions 8.2 Database permissions 8.3 Public &amp; Private Repositories", " 8 Security &amp; Private Data 8.1 File-level permissions 8.2 Database permissions 8.3 Public &amp; Private Repositories 8.3.1 Scrubbing GitHub history Occassionaly files may be committed to your git repository that need to be removed completely. Not just from the current collections of files (i.e., the branch’s head), but from the entire history of the repo. Scrubbing is require typically when (a) a sensitive file has been accidentally commited and pushed to GitHub, or (b) a huge file has bloated your repository and disrupted productivity. The two suitable scrubbing approaches both require the command line. The first is the git-filter-branch command within git, and the second is the BFG repo-cleaner. We use the second approach, which is [recommended by GitHub]; it requires 15 minutes to install and configure from scratch, but then is much easier to develop against, and executes much faster. The bash-centric steps below remove any files from the repo history called ‘monster-data.csv’ from the ‘bloated’ repository. If the file contains passwords, change them immediately. Delete ‘monster-data.csv’ from your branch and push the commit to GitHub. Ask your collaborators to push any outstanding commits to GitHub and delete their local copy of the repo. Once scrubbing is complete, they will re-clone it. Download and install the most recent Java JRE from the Oracle site. Download the most recent jar file from the BFG site to the home directory. Clone a fresh copy of the repository in the user’s home directory. The --mirror argument avoids downloading every file, and downloads only the bookkeeping details required for scrubbing. cd ~ git clone --mirror https://github.com/your-org/bloated.git Remove all files (in any directory) called ‘monster-data.csv’. java -jar bfg-*.jar --delete-files monster-data.csv bloated.git Reflog and garbage collect the repo. cd bloated.git git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive Push your local changes to the GitHub server. git push Delete the bfg jar from the home directory. cd ~ rm bfg-*.jar Ask your collaborators to reclone the repo to their local machine. It is important they restart with a fresh copy, so the once-scrubbed file is not reintroduced into the repo’s history. If the file contains sensitive information, like passwords or PHI, ask GitHub to refresh the cache so the file’s history isn’t accessible through their website, even if the repo is private. 8.3.1.0.1 Resources BFG Repo-Cleaner site Additional BFG instructions GitHub Sensitive Data Removal Policy "],
["automation.html", "9 Automation 9.1 Flow File in R 9.2 Makefile 9.3 SSIS 9.4 cron Jobs &amp; Task Scheduler 9.5 Sink Log Files", " 9 Automation 9.1 Flow File in R 9.2 Makefile 9.3 SSIS 9.4 cron Jobs &amp; Task Scheduler 9.5 Sink Log Files "],
["scaling-up.html", "10 Scaling Up 10.1 Data Storage 10.2 Data Processing", " 10 Scaling Up 10.1 Data Storage Local File vs Conventional Database vs Redshift Usage Cases 10.2 Data Processing R vs SQL R vs Spark "],
["collaboration.html", "11 Parallel Collaboration 11.1 Social Contract 11.2 Code Reviews 11.3 Remote 11.4 Loose Notes", " 11 Parallel Collaboration 11.1 Social Contract Issues Organized Commits &amp; Coherent Diffs Branch &amp; Merge Strategy 11.2 Code Reviews Daily Reviews of PRs Periodic Reviews of Files 11.3 Remote Headset &amp; sharing screens 11.4 Loose Notes 11.4.1 GitHub Review your diffs before committing. Check for things like accidental deletions and debugging code that should be deleted (or at least commented out). Keep chatter to a minimum, especially on projects with 3+ people being notified of every issue post. When encountering a problem, Take as much ownership as reasonable. Don’t merely report there’s an error. If you can’t figure it out, ask the question and describe it well. what low-level file &amp; line of code threw the error. how you have tried to solve it. If there’s a questionable line/chunk of code, trace its origin. Not for the sake of pointing the finger at someone, but for the sake of understanding its origin and history. 11.4.2 Common Code This involves code/files that multiple people use, like the REDCap arches. Run the file before committing it. Run common downstream files too (e.g., if you make a change to the arch, also run the funnel). If an upstream variable name must change, alert people. Post a GitHub issue to announce it. Tell everyone, and search the repo (ctrl+shift+f in RStudio) to alert specific people who might be affected. "],
["document.html", "12 Documentation 12.1 Team-wide 12.2 Project-specific 12.3 Dataset Origin &amp; Structure 12.4 Issues &amp; Tasks 12.5 Flow Diagrams 12.6 Setting up new machine", " 12 Documentation 12.1 Team-wide 12.2 Project-specific 12.3 Dataset Origin &amp; Structure 12.4 Issues &amp; Tasks 12.5 Flow Diagrams 12.6 Setting up new machine (example) "],
["style-guide.html", "13 Style Guide 13.1 Readability 13.2 ggplot2", " 13 Style Guide Using a consistent style across your projects can increase the overhead as your data science team discusses options, decides on a good choice, and develops in compliant code. But like in most themes in this document, the cost is worth the effort. Unforced code errors are reduced when code is consistent, because mistake-prone styles are more apparent. For the most part, our team follows the tidyverse style. Here are some additional conventions we attempt to follow. Many of these were inspired by (Francesco Balena 2005). 13.1 Readability 13.1.1 Abbreviations Try to avoid abbreviations. Different people tend to shorten words differently; this variability increases the chance that people reference the wrong variable. At very least, it wastes time trying to remember if subject_number, subject_num, or subject_no was used. The Consistency section describes how this can reduce errors and increase efficiency. However, some terms are too long to reasonably use without shortening. We make some exceptions, such as the following scenarios: humans commonly use the term orally. For instance, people tend to say “OR” instead of “operating room”. your team has agreed on set list of abbreviations. The list for our CDW team includes: appt, cdw, cpt, drg (stands for diagnosis-related group), dx, hx, icd, and pt. When your team choose terms (e.g., ‘apt’ vs ‘appt’), try to use a standard vocabulary, such as MedTerms Medical Dictionary. 13.1.2 Number The word ‘number’ is ambiguous, especially in data science. Try for more specific terms like ‘count’, ‘id’, ‘index’, and ‘tally’. 13.2 ggplot2 The expressiveness of ggplot2 allows someone to quickly develop precise scientific graphics. One graph can be specified in many equivalent styles, which increases the opportunity for confusion. We formalized much of this style while writing a textbook for introductory statistics; the 200+ graphs and their code is publically available. 13.2.1 Order of commands ggplot2 is essentially a collection of functions combined with the + operator. Publication graphs common require at least 20 functions, which means the functions can sometimes be redundant or step on each other toes. The family of functoins should follow a consistent order ideally starting with the more important structural functions and ending with the cosmetic functions. Our preference is: ggplot2() is the primary function to specify the default dataset and aesthetic mappings. Many arguments can be passed to aes(), and we prefer to follow an order consistent with the scale_*() order below. geom_*() creates the geometric elements that reperesent the data. Unlike most categories in this list, the order matters. Geoms specified first are drawn first, and therefore can be obscured by subsequent geoms. scale_*() describes how a dimension of data (specified in aes()) is translated into a visual element. We specify the dimensions in descending order of (typical) importance: x, y, group, color, fill, size, radius, alpha, shape, linetype. coord_*() facet_*() and label_*() theme() labs() 13.2.2 Gotchas Here are some common mistakes we see not-so-infrequently (even sometimes in our own code). Call coord_*() to restrict the plotted x/y values, not scale_*() or lims()/xlim()/ylim(). coord_*() zooms in on the axes, so extreme values essentially fall off the page; in contrast, the latter three functions essentially remove the values from the dataset. The distinction does not matter for a simple bivariate scatterplot, but likely will mislead you and the viewer in two common scenarios. First, a call to geom_smooth() (e.g., that overlays a loess regression curve) ignore the extreme values entirely; consequently the summary location will be misplaced and its standard errors too tight. Second, when a line graph or spaghetti plots contains an extreme value, it is sometimes desirable to zoom in on the the primary area of activity; when calling coord_*(), the trend line will leave and return to the plotting panel (which implies points exist which do not fit the page), yet when calling the others, the trend line will appear interrupted, as if the extreme point is a missing value. "],
["publication.html", "14 Publishing Results 14.1 To Other Analysts 14.2 To Researchers &amp; Content Experts 14.3 To Technical-Phobic Audiences", " 14 Publishing Results 14.1 To Other Analysts 14.2 To Researchers &amp; Content Experts 14.3 To Technical-Phobic Audiences "],
["testing-and-validation.html", "15 Testing, Validation, &amp; Defensive Programming 15.1 Testing Functions 15.2 Defensive Programming 15.3 Validator", " 15 Testing, Validation, &amp; Defensive Programming 15.1 Testing Functions 15.2 Defensive Programming Throwing errors 15.3 Validator Benefits for Analysts Benefits for Data Collectors "],
["troubleshooting.html", "16 Troubleshooting and Debugging 16.1 Finding Help 16.2 Debugging", " 16 Troubleshooting and Debugging 16.1 Finding Help Within your group (eg, Thomas and REDCap questions) Within your university (eg, SCUG) Outside (eg, Stack Overflow; GitHub issues) 16.2 Debugging traceback(), browser(), etc "],
["workstation.html", "17 Workstation 17.1 Required Installation 17.2 Recommended Installation 17.3 Optional Installation 17.4 Ubuntu Installation 17.5 Asset Locations 17.6 Administrator Installation 17.7 Installation Troubleshooting 17.8 Retired Tools", " 17 Workstation We believe it is important to keep software updated and consistent across workstations in your project. This material was originally posted at https://github.com/OuhscBbmc/RedcapExamplesAndPatterns/blob/master/DocumentationGlobal/ResourcesInstallation.md. It should help establish our tools on a new development computer. 17.1 Required Installation The order matters. 17.1.1 R R is the centerpiece of the analysis. Every few months, you’ll need to download the most recent version. {added Sept 2012} 17.1.2 RStudio RStudio Desktop is the IDE (integrated design interface) that you’ll use to interact with R, GitHub, Markdown, and LaTeX. Updates can be checked easily through the menus Help -&gt; Check for updates. 17.1.3 Installing R Packages Dozens of R Packages will need to be installed. Choose between one of the two related scripts. It will install from our list of packages that our data analysts typically need. The script installs a package only if it’s not already installed; also an existing package is updated if a newer version is available. Create a new ‘personal library’ if it prompts you. It takes at least fifteen minutes, so start it before you go to lunch. The list of packages will evolve over time, so please help keep the list updated. To install our frequently-used packages, run the following snippet. The first lines installs an important package. The second line calls the online Gist, which defines the package_janitor_remote() function. The final line calls the function (and passes a specific CSV of packages)5. if( !base::requireNamespace(&quot;devtools&quot;) ) utils::install.packages(&quot;devtools&quot;) devtools::source_gist(&quot;2c5e7459b88ec28b9e8fa0c695b15ee3&quot;, filename=&quot;package-janitor-bbmc.R&quot;) package_janitor_remote( &quot;https://raw.githubusercontent.com/OuhscBbmc/RedcapExamplesAndPatterns/master/utility/package-dependency-list.csv&quot; ) Some of our projects require specialized packages that are not typically used. In these cases, we will develop the git repo as an R package that includes a proper DESCRIPTION file. See RAnalysisSkeleton for an example. When the project is opened in RStudio, update_packages_addin() in OuhscMunge will find the DESCRIPTION file and install the package dependencies. if( !base::requireNamespace(&quot;remotes&quot; ) ) utils::install.packages(&quot;remotes&quot;) if( !base::requireNamespace(&quot;OuhscMunge&quot;) ) remotes::install_github(&quot;OuhscMunge&quot;) OuhscMunge::update_packages_addin() 17.1.4 Updating R Packages Several R packages will need to be updated every weeks. Unless you have been told not to (because it would break something -this is rare), periodically update the packages by executing the following code update.packages(checkBuilt=TRUE). 17.1.5 GitHub GitHub registration is necessary to push modified files to the repository. First, register a free user account, then tell the repository owner your exact username, and they will add you as a collaborator (e.g., to https://github.com/OuhscBbmc/RedcapExamplesAndPatterns). 17.1.6 GitHub for Windows Client GitHub for Windows Client does the basic tasks a little easier than the git features built into RStudio. Occasionally, someone might need to use git form the command line to fix problems. 17.2 Recommended Installation The order does not matter. ODBC Driver for SQL Server is for connecting to the token server, if your institution is using one. As of this writing, version 17 is the most recent driver version. See if a new one exists. {updated Apr 2018} R Tools for Windows is necessary to build some packages in development hosted on GitHub. {added Feb 2017} Notepad++ is a text editor that allows you look at the raw text files, such as code and CSVs. For CSVs and other data files, it is helpful when troubleshooting (instead of looking at the file through Excel, which masks &amp; causes some issues). {added Sept 2012} Atom is a text editor, similar to Notepad++. Notepad++ appears more efficient opening large CSVs. Atom is better suited when editing a lot of files in a repository. For finding and replacing across a lot of files, it is superior to Notepad++ and RStudio; it permits regexes and has a great GUI preview of the potential replacements. Productivity is enhanced with the following Atom packages: Sublime Style Column Selection: Enable Sublime style ‘Column Selection’. Just hold ‘alt’ while you select, or select using your middle mouse button. atom-language-r allows Atom to recognize files as R. This prevents spell checking indicators and enable syntax highlighting. When you need to browse through a lot of scattered R files quickly, Atom’s tree panel (on the left) works well. An older alternative is language-r. language-csv: Adds syntax highlighting to CSV files. The highlighting is nice, and it automatically disables spell checking lines. atom-beautify: Beautify HTML, CSS, JavaScript, PHP, Python, Ruby, Java, C, C++, C#, Objective-C, CoffeeScript, TypeScript, Coldfusion, SQL, and more in Atom. atom-wrap-in-tag: wraps tag around selection; just select a word or phrase and hit Alt + Shift + w. minimap: A preview of the full source code (in the right margin). script: Run scripts based on file name, a selection of code, or by line number. git-plus: Do git things without the terminal (I don’t think this is necessary anymore). The packages can be installed through Atom, or through the apm utility in the command line: apm install sublime-style-column-selection atom-language-r language-csv atom-beautify atom-wrap-in-tag minimap script And the following settings keep files consistent among developers. File | Settings | Editor | Tab Length: 2 (As opposed to 3 or 4, used in other conventions) File | Settings | Editor | Tab Type: soft (This inserts 2 spaces instead of a tab when ‘Tab’ is pressed) SQL Server Management Studio (SSMS) is an easy way to access the database and write queries (and transfer the SQL to an R file). It’s not required for the REDCap API, but it’s usually necessary when integrating REDCap with other databases. Note: here are some non-default changes that facilitate our workflow. The first two help when we save the database structure (not data) on GitHub, so we can easily track/monitor the structural changes over time. The tabs options keeps things consistent between editors. In the SSMS ‘Tools | Options’ dialog box: ‘SQL Server Object Explorer’ | ‘Scripting’ | ‘Include descriptive headers’: False ‘SQL Server Object Explorer’ | ‘Scripting’ | ‘Script extended properties’: False ‘Text Editor’ | ‘All Languages’ | ‘Tabs’ | ‘Tab size’: 2 ‘Text Editor’ | ‘All Languages’ | ‘Tabs’ | ‘Indent size’: 2 ‘Text Editor’ | ‘All Languages’ | ‘Tabs’ | ‘Insert Spaces’: selected These don’t affect the saved files, but make life easier. The first makes the result font bigger. ‘Environment’ | ‘Fonts and Colors’ | ‘Show settings for: Grid Results’ | ‘Size’: 10 ‘Query Results’ | ‘SQL Server’ | ‘Results to Grid’ | ‘Include column headers when copying or saving the results’: checked ‘Designers’ | ‘Table and Database Designers’ | ‘Prevent saving changes that require table-recreation’: unchecked ‘Text Editor’ | ‘Editor Tab and Status Bar’ | ‘Tab Text’ | ‘Include Server Name’: false ‘Text Editor’ | ‘Editor Tab and Status Bar’ | ‘Tab Text’ | ‘Include Database Name’: false ‘Text Editor’ | ‘Editor Tab and Status Bar’ | ‘Tab Text’ | ‘Include Login Name’: false ‘Text Editor’ | ‘All Languages’ | ‘General’ | ‘Line Numbers’: true For more details, see setting-up-dev-machine.md (in a private repo that’s restricted to BBMC members). Pulse Secure is VPN client for OUHSC researchers. It’s not required for the REDCap API, but it’s usually necessary to communicate with other campus data sources. 17.3 Optional Installation The order does not matter. Git command-line utility enables some advanced operations that the GitHub client doesn’t support. Use the default installation options, except these preferences of ours: Nano is the default text editor. GitLab SSL Certificate isn’t software, but still needs to be configured. Talk to Will for the server URL and the *.cer file. Save the file in something like ~/keys/ca-bundle-gitlab.cer Associate the file with git config --global http.sslCAInfo ...path.../ca-bundle-gitlab.cer (but replace ...path...). MiKTeX is necessary only if you’re using knitr or Sweave to produce LaTeX files (and not just markdown files). It’s a huge, slow installation that can take an hour or two. {added Sept 2012} LibreOffice Calc is an alternative to Excel. Unlike it Excel, it doesn’t guess much with formatting (which usually mess up things, especially dates). Visual Studio Code is an extensible text editor that runs on Windows and Linux, similar to Atom (described above). It’s much lighter than the full Visual Studio. Like Atom, it supports browsing through the directory structure, replacing across files, interaction with git, and previewing markdown. Currently, it supports searching CSVs better than Atom. Productivity is enhanced with the following extensions: {added Dec 2018} Excel Viewer isn’t a good name, but I’ve liked the capability. It displays CSVs and other files in a grid. {added Dec 2018} Rainbow CSV color codes the columns, but still allows you to see and edit the raw plain-text file. {added Dec 2018} SQL Server allows you to execute against a database, and view/copy/save the grid results. It doesn’t replicate all SSMS features, but is nice as your scanning through files. {added Dec 2018} Code Spell Checker produces green squiggly lines under words not in its dictionary. You can add words to your user dictionary, or a project dictionary. These extensions can be installed by command line. code --list-extensions code --install-extension GrapeCity.gc-excelviewer code --install-extension mechatroner.rainbow-csv code --install-extension ms-mssql.mssql code --install-extension streetsidesoftware.code-spell-checker pandoc converts files from one markup format into another. {added Sept 2012} 17.4 Ubuntu Installation Ubuntu desktop 19.04 follows these instructions for the R and RStudio and required these debian packages to be installed before the R packages. The --yes option avoids manual confirmation for each line, so you can copy &amp; paste this into the terminal. Add the following to the sources with sudo nano /etc/apt/sources.list. The ‘eoan’ version may be updated; The ‘metrocast’ part could be modified too from this list. I found it worked better for a new Ubuntu release than ‘cloud.r-project.org’. deb http://mirror.metrocast.net/ubuntu/ eoan main deb-src http://mirror.metrocast.net/ubuntu/ eoan main deb http://mirror.metrocast.net/ubuntu/ eoan-backports main restricted universe This next block can be copied and pasted (ctrl-shift-v) into the console entirely. Or lines can be pasted individual (without the ( function install-packages { line, or the last three lines). ( function install-packages { ### Add the key, update the list, then install base R. sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo apt-get update sudo apt-get install r-base sudo apt-get install r-base-dev ### Git sudo apt-get install git-core git config --global user.email &quot;wibeasley@hotmail.com&quot; git config --global user.name &quot;Will Beasley&quot; git config --global credential.helper &#39;cache --timeout=3600000&#39; ### Ubuntu &amp; Bioconductor packages that are indirectly needed for packages and BBMC scripts # Supports the `locate` command in bash sudo apt-get install mlocate # The genefilter package is needed for &#39;modeest&#39; on CRAN. # No longer a modeest dependency: Rscript -e &#39;BiocManager::install(&quot;genefilter&quot;)&#39; ### CRAN packages that are also on the Ubuntu repositories # The &#39;xml2&#39; package; https://CRAN.R-project.org/package=xml2 sudo apt-get --yes install libxml2-dev r-cran-xml # The &#39;curl&#39; package, and others; https://CRAN.R-project.org/package=curl sudo apt-get --yes install libssl-dev libcurl4-openssl-dev # The &#39;udunits2&#39; package: https://cran.r-project.org/web/packages/udunits2/index.html sudo apt-get --yes install libudunits2-dev # The &#39;odbc&#39; package: https://github.com/r-dbi/odbc#linux---debian--ubuntu sudo apt-get --yes install unixodbc-dev tdsodbc odbc-postgresql libsqliteodbc # The &#39;rgl&#39; package; https://stackoverflow.com/a/39952771/1082435 sudo apt-get --yes install libcgal-dev libglu1-mesa-dev # The &#39;magick&#39; package; https://docs.ropensci.org/magick/articles/intro.html#build-from-source sudo apt-get --yes install &#39;libmagick++-dev&#39; # To compress vignettes when building a package; https://kalimu.github.io/post/checklist-for-r-package-submission-to-cran/ sudo apt-get --yes install qpdf # The &#39;pdftools&#39; and &#39;Rpoppler&#39; packages, which involve PDFs sudo apt-get --yes install libpoppler-cpp-dev libpoppler-glib-dev # The &#39;sys&#39; package sudo apt-get --yes install libapparmor-dev # The &#39;sf&#39; and other spatial packages: https://github.com/r-spatial/sf#ubuntu; https://github.com/r-spatial/sf/pull/1208 sudo apt-get --yes install libudunits2-dev libgdal-dev libgeos-dev libproj-dev libgeos++-dev # For Cairo package, a dependency of Shiny &amp; plotly; https://gykovacsblog.wordpress.com/2017/05/15/installing-cairo-for-r-on-ubuntu-17-04/ sudo apt-get --yes install libcairo2-dev # &#39;rJava&#39; and others; https://www.r-bloggers.com/installing-rjava-on-ubuntu/ sudo apt-get --yes install default-jre default-jdk sudo R CMD javareconf sudo apt-get --yes install r-cran-rjava # For reprex and sometimes ssh keys; https://github.com/tidyverse/reprex#installation sudo apt-get --yes install xclip # gifski -apparently the rust compiler is necessary sudo apt-get --yes install cargo # For databases sudo apt-get --yes install sqlite sqliteman sudo apt-get --yes install postgresql postgresql-contrib pgadmin3 # pandoc sudo apt install pandoc } install-packages ) The version of pandoc from the Ubuntu repository may be delayed. To install the latest version, download the .deb file then install from the same directory. Finally, verify the version. sudo dpkg -i pandoc-* pandoc -v The Postman native app for Ubuntu is installed through snap, which is updated daily automatically. snap install postman 17.5 Asset Locations GitHub repository https://github.com/OuhscBbmc/RedcapExamplesAndPatterns {added Sept 2012} File server directory Ask your PI. For Peds, it’s typically on the “S” drive. SQL Server Database Ask Thomas, Will or David REDCap database Ask Thomas, Will or David. It is a http url, and we’re trying not to publicize its value. ODBC UserDsn The name depends on your specific repository, and SQL Server database. Ask Thomas, Will or David for how to set it up. 17.6 Administrator Installation MySQL Workbench is useful occassionly for REDCap admins. Postman Native App is useful for developing with the API and has replaced the Chrome app. If that’s not possible, a web client is available as well. With either program, do not access any PHI. 17.7 Installation Troubleshooting Git: Will Beasley resorted to this workaround Sept 2012: http://stackoverflow.com/questions/3431361/git-for-windows-the-program-cant-start-because-libiconv2-dll-is-missing. And then he copied the following four files from D:/Program Files/msysgit/mingw/bin/ to D:/Program Files/msysgit/bin/: (1) libiconv2.dll, (2) libcurl-4.dll, (3) libcrypto.dll, and (4) libssl.dll. (If you install to the default location, you’ll move instead from C:/msysgit/mingw/bin/ to C:/msysgit/bin/) {added Sept 2012} Git: On a different computer, Will Beasley couldn’t get RStudio to recognize msysGit, so installed the Full installer for official Git for Windows 1.7.11 from (http://code.google.com/p/msysgit/downloads/list) and switched the Git Path in the RStudio Options. {added Sept 2012} RStudio If something goes wrong with RStudio, re-installing might not fix the issue, because your personal preferences aren’t erased. To be safe, you can be thorough and delete the equivalent of C:\\Users\\wibeasley\\AppData\\Local\\RStudio-Desktop\\. The options settings are stored (and can be manipulated) in this extentionless text file: C:\\Users\\wibeasley\\AppData\\Local\\RStudio-Desktop\\monitored\\user-settings\\user-settings. {added Sept 2012} 17.8 Retired Tools We previously installed this software in this list. Most have been replaced by software above that’s either newer or more natural to use. msysGit allows RStudio to track changes and commit &amp; sync them to the GitHub server. Connect RStudio to GitHub repository. I moved this to optional (Oct 14, 2012) because the GitHub client (see above) does almost everything that the RStudio plugin does; and it does it a little better and a little more robust; and its installation hasn’t given me problems. {added Oct 2012} Starting in the top right of RStudio, click: Project -&gt; New Project -&gt; Create Project from Version Control -&gt; Git {added Sept 2012} An example of a repository URL is https://github.com/OuhscBbmc/RedcapExamplesAndPatterns. Specify a location to save (a copy of) the project on your local computer. {added Sept 2012} CSVed is a lightweight program for viewing data files. It fits somewhere between a text editor and Excel. SourceTree is a rich client that has many more features than the GitHub client. I don’t recommend it for beginners, since it has more ways to mess up things. But for developers, it nicely fills a spot in between the GitHub client and command-line operations. The branching visualization is really nice too. Unfortunately and ironically, it doesn’t currently support Linux. {added Sept 2014}. git-cola is probably the best GUI for Git supported on Linux. It’s available through the official Ubuntu repositories with apt-get (also see this). The branch visualization features are in a different, but related program, ‘git dag’. {added Sept 2014} GitHub for Eclipse is something I discourage for a beginner, and I strongly recommend you start with RStudio (and GitHub Client or the git capabilities within RStudio) for a few months before you even consider Eclipse. It’s included in this list for the sake of completeness. When installing EGit plug-in, ignore eclipse site and check out this youtube video:http://www.youtube.com/watch?v=I7fbCE5nWPU. Color Oracle simulates the three most common types of color blindness. If you have produce a color graph in a report you develop, check it with Color Oracle (or ask someone else too). If it’s already installed, it takes less than 10 second to check it against all three types of color blindness. If it’s not installed, extra work may be necessary if Java isn’t already installed. When you download the zip, extract the ColorOracle.exe program where you like. {added Sept 2012} As an alternative to the Gist, run the local R script install-packages.R (located in the utility/ directory) that lives in this repository. The workhorse of this function is OuhscMunge::package_janitor().↩︎ "],
["tools.html", "18 Considerations when Selecting Tools 18.1 General 18.2 Languages 18.3 R Packages 18.4 Database", " 18 Considerations when Selecting Tools 18.1 General 18.1.1 The Component’s Goal While discussing the advantages and disadvantages of tools, a colleague once said, “Tidyverse packages don’t do anything that I can’t already do in Base R, and sometimes it even requires more lines of code”. Regardless if I agree, I feel these two points are irrelevant. Sometimes the advantage of a tool isn’t to expand existing capabilities, but rather to facilitate development and maintenance for the same capability. Likewise, I care less about the line count, and more about the readability. I’d prefer to maintain a 20-line chunk that is familiar and readable than a 10-line chunk with dense phrases and unfamiliar functions. The bottleneck for most of our projects is human time, not execution time. 18.1.2 Current Skillset of Team 18.1.3 Desired Future Skillset of Team 18.1.4 Skillset of Audience 18.2 Languages 18.3 R Packages When developing a codebase used by many people, choose packages both on their functionality, as well as their ease of installation and maintainability. For example, the rJava package is a powerful package that allows R package developers to leverage the widespread Java framework and many popular Java packages. However, installing Java and setting the appropriate path or registry settings can be error-prone, especially for non-developers. Therefore when considering between two functions with comparable capabilities (e.g., xlsx::read.xlsx() and readxl::read_excel()), avoid the package that requires a proper installation and configuration of Java and rJava. If the more intensive choice is required (say, you need to a capability in xslx missing from readxl), take: 20 minutes to start a markdown file that enumerates the package’s direct and indirect dependencies that require manual configuration (e.g., rJava and Java), where to download them, and the typical installation steps. 5 minutes to create a GitHub Issue that (a) announces the new requirement, (b) describes who/what needs to install the requirement, (c) points to the markdown documentation, and (d) encourages teammates to post their problems, recommendations, and solutions in this issue. We’ve found that a dedicated Issue helps communicate that the package dependency necessitates some intention and encourages people to assist other people’s troubleshooting. When something potentially useful is posted in the Issue, move it to the markdown document. Make sure the document and the issue hyperlink to each other. 15 minutes every year to re-evaluate the landscape. Confirm that the package is still actively maintained, and that no newer (and easily- maintained) package offers the desired capability.6 If better fit now exists, evaluate if the effort to transition to the new package is worth the benefit. Be more willing to transition is the project is relatively green, and more development is upcoming. Be more willing to transition if the transition is relatively in-place, and will not require much modification of code or training of people. Finally, consider how much traffic passes through the dependency A brittle dependency will not be too disruptive if isolated in a downstream analysis file run by only one statistician. On the other hand, be very protective in the middle of the pipeline where typically most of your team runs. 18.4 Database Ease of installation &amp; maintenance Support from IT –which database engine are they most comfortable supporting. Integration with LDAP, Active Directory, or Shibboleth. Warehouse vs transactional performance In this case, the openxlsx package is worth consideration because it writes to an Excel file, but uses a C++ library, not a Java library.↩︎ "],
["team.html", "19 Growing a Team 19.1 Recruiting 19.2 Training to Data Science 19.3 Bridges Outside the Team", " 19 Growing a Team 19.1 Recruiting 19.2 Training to Data Science Starting with a Researcher Starting with a Statistician Starting with a DBA Starting with a Software Developer 19.3 Bridges Outside the Team Monthly User Groups Annual Conferences "],
["appendix-appendix.html", "(APPENDIX) Appendix", " (APPENDIX) Appendix "],
["git-github.html", "20 Git &amp; GitHub 20.1 for Code Development 20.2 for Collaboration 20.3 for Stability", " 20 Git &amp; GitHub 20.1 for Code Development Jenny Bryan and Jim Hester have published a thorough description of using Git from a data scientist’s perspective (Happy Git and GitHub for the useR), and we recommend following their guidance. It is consistent with our approach, with a few exceptions noted below. A complementary resource is Team Geek, which has insightful advice for the human and collaborative aspects of version control. Other Resources 1. Setting up a CI/CD Process on GitHub with Travis CI. Travis-CI blob from August 2019. 20.2 for Collaboration Somewhat separate from it’s version control capabilities, GitHub provides built-in tools for coordinating projects across people and time. This tools revolves around GitHub Issues, which allow teammates to track issues assigned to them and others search if other teammates have encountered similar problems that their facing now (e.g., the new computer can’t install the rJava package). There’s nothing magical about GitHub issues, but if you don’t use them, consider using a similar or more capable tools like those offered by Atlassian, Asana, Basecamp, and many others. Here are some tips from our experiences with projects involving between 2 and 10 statisticians are working with an upcoming deadline. If you create an error that describes a problem blocking your progress, include both the raw text (e.g., error: JAVA_HOME cannot be determined from the Registry) and possibly a screenshot. The text allows the problem to be more easily searched by people later; the screenshot usually provides extra context that allows other to understand the situation and help more quickly. Include enough broad context and enough specific details that teammates can quickly understand the problem. Ideally they can even run your code and debug it. Good recommendations can be found in the Stack Overflow posts, ‘How to make a great R reproducible example’ and ‘How do I ask a good question?’. The issues don’t need to be as thorough, because your teammates start with more context than a Stack Overflow reader. We typically include a description of the problem or fishy behavior. the exact error message (or a good description of the fishy behavior). a snippet of the 1-10 lines of code suspected of causing the problem. a link to the code’s file (and ideally the line number, such as https://github.com/OuhscBbmc/REDCapR/blob/master/R/redcap-version.R#L40) so the reader can hop over to the entire file. references to similar GitHub Issues or Stack Overflow questions that could aid troubleshooting. 20.3 for Stability Review Git commits closely No unintended functional difference (e.g., !match accidentally changed to match). No PHI snuck in (e.g., a patient ID used while isolating and debugging). The metadata format didn’t change (e.g., Excel sometimes changes the string ‘010’ to the number ‘10’). "],
["snippets.html", "21 Snippets 21.1 Reading External Data 21.2 Grooming", " 21 Snippets 21.1 Reading External Data 21.1.1 Reading from Excel Background: Avoid Excel for so many reasons. But if there isn’t another good option, be protective. readxl::read_excel() allows you to specify column types, but not column order. The names of col_types is ignored by readxl::read_excel(). To defend against roamining columns (e.g., the files changed over time), tesit::assert() that the order is what you expect. Last Modified: 2019-12-12 by Will # ---- declare-globals --------------------------------------------------------- config &lt;- config::get() # cat(sprintf(&#39; `%s` = &quot;text&quot;,\\n&#39;, colnames(ds)), sep=&quot;&quot;) # &#39;text&#39; by default --then change where appropriate. col_types &lt;- c( `Med Rec Num` = &quot;text&quot;, `Admit Date` = &quot;date&quot;, `Tot Cash Pymt` = &quot;numeric&quot; ) # ---- load-data --------------------------------------------------------------- ds &lt;- readxl::read_excel( path = config$path_admission_charge, col_types = col_types # sheet = &quot;dont-use-sheets-if-possible&quot; ) testit::assert( &quot;The order of column names must match the expected list.&quot;, names(col_types) == colnames(ds) ) 21.1.2 Removing Trailing Comma from Header Background: Ocassionally a Meditech Extract will have an extra comma at the end of the 1st line. For each subsequent line, readr:read_csv() appropriately throws a new warning that it is missing a column. This warning flood can mask real problems. Explanation: This snippet (a) reads the csv as plain text, (b) removes the final comma, and (c) passes the plain text to readr::read_csv() to convert it into a data.frame. Instruction: Modify Dx50 Name to the name of the final (real) column. Real Example: truong-pharmacist-transition-1 (Accessible to only CDW members.) Last Modified: 2019-12-12 by Will # The next two lines remove the trailing comma at the end of the 1st line. raw_text &lt;- readr::read_file(path_in) raw_text &lt;- sub(&quot;^(.+Dx50 Name),&quot;, &quot;\\\\1&quot;, raw_text) ds &lt;- readr::read_csv(raw_text, col_types=col_types) 21.2 Grooming 21.2.1 Correct for misinterpreted two-digit year Background: Sometimes the Meditech dates are specified like 1/6/54 instead of 1/6/1954. readr::read_csv() has to choose if the year is supposed to be ‘1954’ or ‘2054’. A human can use context to guess a birth date is in the past (so it guesses 1954), but readr can’t (so it guesses 2054). For avoid this and other problems, request dates in an ISO-8601 format. Explanation: Correct for this in a dplyr::mutate() clause; compare the date value against today. If the date is today or before, use it; if the day is in the future, subtract 100 years. Instruction: For future dates such as loan payments, the direction will flip. Last Modified: 2019-12-12 by Will ds %&gt;% dplyr::mutate( dob = dplyr::if_else(dob &lt;= Sys.Date(), dob, dob - lubridate::years(100)) ) "],
["presentations.html", "22 Presentations 22.1 CDW 22.2 REDCap 22.3 Reproducible Research &amp; Visualization 22.4 Data Management 22.5 GitHub 22.6 Software 22.7 Architectures 22.8 Components", " 22 Presentations Here is a collection of presentations by the BBMC and friends that may help demonstrate concepts discussed in the previous chapters. 22.1 CDW prairie-outpost-public: Documentation and starter files for OUHSC’s Clinical Data Warehouse. OUHSC CDW 22.2 REDCap REDCap Systems Integration. REDCap Con 2015, Portland, Oregon. Literate Programming Patterns and Practices with REDCap REDCap Con 2014, Park City, Utah. Interacting with the REDCap API using the REDCapR Package REDCap Con 2014, Park City, Utah. Optimizing Study Management using REDCap, R, and other software tools. SCUG 2013. 22.3 Reproducible Research &amp; Visualization Building pipelines and dashboards for practitioners: Mobilizing knowledge with reproducible reporting. Displaying Health Data Colloquium 2018, University of Victoria. Interactive reports and webpages with R &amp; Shiny. SCUG 2015. Big data, big analysis: a collaborative framework for multistudy replication. Conventional of Canadian Psychological Association, Victoria BC, 2016. WATS: wrap-around time series: Code to accompany WATS Plot article, 2014. 22.4 Data Management BBMC Validator: catch and communicate data errors. SCUG 2016. Text manipulation with Regular Expressions, Part 1 and Part 2. SCUG 2016. Time and Effort Data Synthesis. SCUG 2015. 22.5 GitHub Scientific Collaboration with GitHub. OU Bioinformatics Breakfast Club 2015. 22.6 Software REDCapR: Interaction Between R and REDCap. OuhscMunge: Data manipulation operations commonly used by the Biomedical and Behavioral Methodology Core within the Department of Pediatrics of the University of Oklahoma Health Sciences Center. codified: Produce standard/formalized demographics tables. usnavy billets: Optimally assigning naval officers to billets. 22.7 Architectures Linear Pipeline of the R Analysis Skeleton . Many-to-many Pipeline of the R Analysis Skeleton . Immunization transfer . IALSA: A Collaborative Modeling Framework for Multi-study Replication . POPS: Automated daily screening eligibility for rare and understudied prescriptions. . 22.8 Components Customizing display tables: using css with DT and kableExtra. SCUG 2018. yaml and expandable trees that selectively show subsets of hierarchy, 2017. "],
["scratch-pad.html", "23 Scratch Pad of Loose Ideas 23.1 Chapters &amp; Sections to Form", " 23 Scratch Pad of Loose Ideas 23.1 Chapters &amp; Sections to Form Tools to Consider tidyverse odbc ggplot2 use factors for explanatory variables when you want to keep the order consistent across graphs. (genevamarshall) styles variable names: within a variable name order from big to small terms (lexigraphical scoping) (thomasnwilson) public reports (and dashboards) when developing a report for a external audience (ie, people outside your immediate research team), choose one or two pals who are unfamilar with your aims/methods as an impromptu focus group. Ask them what things need to be redesigned/reframed/reformated/further-explained. (genevamarshall) plots plot labels/axes variable names units of measurement (eg, proportion vs percentage on the y axis) documentation - bookdown Bookdown has worked well for us so far. It’s basically independent markdown documents stored on a dedicated git repo. Then you click “build” in RStudio and it converts all the markdown files to static html files. Because GitHub is essentially serving as the backend, everyone can make changes to sections and we don’t have to be too worried about Here’s a version that’s hosted publicly, but I tested that it can be hosted on our shared file server. (It’s possible because the html files are so static.) If this is what you guys want for OU’s collective CDW, please tell me: who you want to be able to edit the documents without review. I’ll add them to the GitHub repo. who you want to be able to view the documents. I’ll add them to a dedicate file server space. https://ouhscbbmc.github.io/data-science-practices-1/workstation.html#installation-required I was thinking that each individual database gets it own chapter. The BBMC has ~4 databases in this sense: a Centricity staging database, a GECB staging database, the central warehouse, and the (fledgling) downstream OMOP database. Then there are ~3 sections within each chapter: (a) a black-and-white description of the tables, columns, &amp; indexes (written mostly for consumers), (b) recommendations how to use each table (written mostly for consumers), and (c) a description of the ETL process (written mostly for developers &amp; admins). My proposal uses GitHub and Markdown because they’re so universal (no knowledge of R is required –really you could write it with any text editor &amp; commit, and let someone else click “build” in RStudio on their machine). But I’m very flexible on all this. I’ll support &amp; contribute to any system that you guys feel will work well across the teams. developing packages R packages by Hadley Wickham http://mangothecat.github.io/goodpractice/ "],
["example-bookdown.html", "24 Example", " 24 Example This intro was copied from the 1st chapter of the example bookdown repo. I’m keeping it temporarily for reference. You can label chapter and section titles using {#label} after them, e.g., we can reference the Intro Chapter. If you do not manually label them, there will be automatic labels anyway Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "],
["acknowledgements.html", "25 Acknowledgements", " 25 Acknowledgements The authors thank all our colleauges for the discussions and experiences about data science that lead to this book. At OUHSC, this includes @adrose, @aggie-dbc, @ARPeters, @Ashley-Jorgensen, @athumann, @atreat1, @caston60, @chanukyalakamsani, @CWilliamsOUHSC, @DavidBard, @evoss1, @genevamarshall, @Maleeha, @man9472, @rmatkins, @sbohora, @vimleshbavadiya, @waleboro, @YuiYamaoka, @yutiantang. Outside the OUHSC, this includes @andkov, @ben519, @cscherrer, @cmodzelewski, @jimquallen, @mhunter1, @probinso, @russelljonas, and @spopovych. "],
["references.html", "26 References", " 26 References Francesco Balena, Giuseppe Dimauro. 2005. Practical Guidelines and Best Practices for Microsoft Visual Basic and Visual c# Developers. 1st ed. Redmond, Washington: Microsoft Press. https://books.google.com/books/about/Practical_Guidelines_and_Best_Practices.html?id=zphQAAAAMAAJ. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2019. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. "]
]
